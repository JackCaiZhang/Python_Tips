{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df7cd54",
   "metadata": {},
   "source": [
    "## 1. TextBlobï¼šå¿«é€Ÿè‡ªç„¶è¯­è¨€å¤„ç†çš„â€œè¢«ä½ä¼°çš„é­”æ³•å¸ˆâ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2675e3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Sentiment(polarity=0.625, subjectivity=0.6)\n",
      "Noun Phrases: ['python', 'automation effortless']\n",
      "Corrected:: Python is a powerful programming language!\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = TextBlob(\"I absolutely love how Python makes automation effortless!\")\n",
    "print(f'Sentiment: {text.sentiment}')\n",
    "print(f'Noun Phrases: {text.noun_phrases}')\n",
    "print(f'Corrected:: {TextBlob('Python is a powrful programing language!').correct()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c3d8a6",
   "metadata": {},
   "source": [
    "## 2. spaCyï¼šä¸ºè®¨åŒæ…¢ä»£ç çš„å¼€å‘è€…å‡†å¤‡çš„å·¥ä¸šçº§ NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3102c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æ¥¼ç›˜ç›¸å…³å®ä½“: [('å¹¿å·', 'GPE'), ('3', 'CARDINAL')]\n",
      "ğŸš‡ åœ°é“çº¿è·¯: ['åœ°é“ 3 å·çº¿']\n",
      "ğŸ· åˆ†ç±»: ['äº¤é€š', 'æ•™è‚²', 'ç‰©ä¸š', 'åŒºä½']\n",
      "ğŸ’¬ æƒ…æ„Ÿå¾—åˆ†: 2\n",
      "ğŸ“ å„å¥å­å¾—åˆ†:\n",
      "\t#1-ä¿åˆ©é›†å›¢åœ¨å¹¿å·å¼€å‘çš„ä¿åˆ©å¤©ç©ºä¹‹åŸä½ç½®å¾ˆå¥½ï¼Œ: 1\n",
      "\t#2-ä½äºå¸‚ä¸­å¿ƒï¼Œ: 0\n",
      "\t#3-ä¸”è·ç¦»åœ°é“ 3 å·çº¿å¾ˆè¿‘ï¼Œ: 1\n",
      "\t#4-å‘¨è¾¹å­¦æ ¡èµ„æºä¸é”™ï¼Œ: 1\n",
      "\t#5-å°±æ˜¯ç‰©ä¸šæœåŠ¡å·®ã€‚: -1\n",
      "ğŸ“˜ æƒ…æ„Ÿ: positive\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.language import Language\n",
    "\n",
    "# åŠ è½½ä¸­æ–‡æ¨¡å‹ï¼ˆéœ€è¦æå‰å®‰è£…ï¼‰\n",
    "# python -m spacy download zh_core_web_sm\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "# æ·»åŠ é»˜è®¤å¥å­åˆ†å‰²å™¨ï¼ˆåŸºäº \"ã€‚ï¼ï¼Ÿ\"ï¼‰\n",
    "if \"sentencizer\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"sentencizer\", first=True)\n",
    "\n",
    "# è¿›ä¸€æ­¥å¢å¼ºä¸­æ–‡åˆ†å¥èƒ½åŠ›ï¼ˆå¤„ç† ï¼Œï¼›ç­‰æƒ…å†µï¼‰\n",
    "@Language.component('custom_seg')\n",
    "def custom_seg(doc):\n",
    "    for i, token in enumerate(doc[:-1]):\n",
    "        # é€—å·ä½†æƒ…æ„Ÿè½¬æŠ˜è¯é™„è¿‘ â†’ åˆ†å¥\n",
    "        if token.text in [\"ï¼Œ\", \",\", \";\", \"ï¼›\"]:\n",
    "            doc[i+1].is_sent_start = True\n",
    "\n",
    "        # å¼ºåˆ¶åˆ†å¥ç‚¹\n",
    "        if token.text in [\"ï¼Ÿ\", \"ï¼\", \"ã€‚\", \"â€¦\"]:\n",
    "            doc[i+1].is_sent_start = True\n",
    "\n",
    "    return doc\n",
    "\n",
    "if 'custom_seg' not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"custom_seg\", after=\"sentencizer\")\n",
    "\n",
    "# ç¤ºä¾‹æ–‡æœ¬\n",
    "text = \"ä¿åˆ©é›†å›¢åœ¨å¹¿å·å¼€å‘çš„ä¿åˆ©å¤©ç©ºä¹‹åŸä½ç½®å¾ˆå¥½ï¼Œä½äºå¸‚ä¸­å¿ƒï¼Œä¸”è·ç¦»åœ°é“ 3 å·çº¿å¾ˆè¿‘ï¼Œå‘¨è¾¹å­¦æ ¡èµ„æºä¸é”™ï¼Œå°±æ˜¯ç‰©ä¸šæœåŠ¡å·®ã€‚\"\n",
    "\n",
    "doc = nlp(text)\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# ===========\n",
    "# 1. å‘½åå®ä½“è¯†åˆ«ï¼ˆæå–åœ°ç‚¹ã€æœºæ„ã€æ¥¼ç›˜åç­‰ï¼‰\n",
    "# ===========\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# ===============================================\n",
    "# 2. åœ°é“çº¿è·¯æå–\n",
    "# ===============================================\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_metro = [{\"TEXT\": \"åœ°é“\"}, {\"IS_DIGIT\": True}, {\"TEXT\": \"å·çº¿\"}]\n",
    "matcher.add(\"METRO_LINE\", [pattern_metro])\n",
    "matches = matcher(doc)\n",
    "metros = [doc[start:end].text for _, start, end in matches]\n",
    "\n",
    "# ===============================================\n",
    "# 3. åˆ†ç±»ï¼ˆäº¤é€šã€æ•™è‚²ã€ç‰©ä¸šã€åŒºä½ç­‰ï¼‰\n",
    "# ===============================================\n",
    "categories = {\n",
    "    \"äº¤é€š\": [\"åœ°é“\", \"å…¬äº¤\", \"å‡ºè¡Œ\"],\n",
    "    \"æ•™è‚²\": [\"å­¦æ ¡\", \"å°å­¦\", \"ä¸­å­¦\", \"æ•™è‚²\"],\n",
    "    \"ç‰©ä¸š\": [\"ç‰©ä¸š\", \"æœåŠ¡\", \"ç®¡ç†\"],\n",
    "    \"åŒºä½\": [\"å¸‚ä¸­å¿ƒ\", \"ä½ç½®\", \"åœ°æ®µ\", \"å‘¨è¾¹\"],\n",
    "}\n",
    "\n",
    "detected_categories = []\n",
    "for cat, kws in categories.items():\n",
    "    if any(k in text for k in kws):\n",
    "        detected_categories.append(cat)\n",
    "\n",
    "# ===============================================\n",
    "# 4. æƒ…æ„Ÿåˆ†æï¼ˆå¼ºåŒ–ç‰ˆ scoring + å¥å­çº§ï¼‰\n",
    "# ===============================================\n",
    "positive_words = [\"å¾ˆå¥½\", \"ä¸é”™\", \"å¾ˆè¿‘\", \"ä¾¿åˆ©\", \"èˆ’æœ\", \"æ»¡æ„\"]\n",
    "negative_words = [\"å·®\", \"ä¸å¥½\", \"ä¸€èˆ¬\", \"ç³Ÿç³•\", \"åµ\", \"å¾ˆè¿œ\"]\n",
    "\n",
    "sentiment_score = 0\n",
    "sentence_scores = []\n",
    "\n",
    "for sent in doc.sents:\n",
    "    s_score = 0\n",
    "    sent_text = sent.text\n",
    "\n",
    "    for w in positive_words:\n",
    "        if w in sent_text:\n",
    "            s_score += 1\n",
    "\n",
    "    for w in negative_words:\n",
    "        if w in sent_text:\n",
    "            s_score -= 1\n",
    "\n",
    "    sentence_scores.append((sent_text, s_score))\n",
    "    sentiment_score += s_score\n",
    "\n",
    "# æœ€ç»ˆæƒ…æ„Ÿ\n",
    "if sentiment_score > 0:\n",
    "    sentiment = \"positive\"\n",
    "elif sentiment_score < 0:\n",
    "    sentiment = \"negative\"\n",
    "else:\n",
    "    sentiment = \"neutral\"\n",
    "\n",
    "# ===============================================\n",
    "# æœ€ç»ˆè¾“å‡º\n",
    "# ===============================================\n",
    "print(\"ğŸ” æ¥¼ç›˜ç›¸å…³å®ä½“:\", entities)\n",
    "print(\"ğŸš‡ åœ°é“çº¿è·¯:\", metros)\n",
    "print(\"ğŸ· åˆ†ç±»:\", detected_categories)\n",
    "print(\"ğŸ’¬ æƒ…æ„Ÿå¾—åˆ†:\", sentiment_score)\n",
    "print(\"ğŸ“ å„å¥å­å¾—åˆ†:\")\n",
    "for idx, (sentence, score) in enumerate(sentence_scores):\n",
    "    print(f'\\t#{idx+1}-{sentence}: {score}')\n",
    "print(\"ğŸ“˜ æƒ…æ„Ÿ:\", sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b212892",
   "metadata": {},
   "source": [
    "## 3. LlamaIndexï¼ˆGPT Indexï¼‰ï¼šæŠŠæ–‡æ¡£å˜æˆå¯æŸ¥è¯¢çš„è®°å¿†ç³»ç»Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed308e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class ArkLLM without an implementation for abstract methods 'achat', 'acomplete', 'astream_chat', 'astream_complete', 'chat', 'metadata', 'stream_chat', 'stream_complete'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msettings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Settings\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mark_llm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArkLLM\n\u001b[1;32m----> 5\u001b[0m Settings\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m ArkLLM(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mep-20251021172124-7hm9t\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 1. åŠ è½½æ–‡æ¡£ï¼ˆæ”¯æŒ docx / pdf / txt / mdï¼‰\u001b[39;00m\n\u001b[0;32m      8\u001b[0m documents \u001b[38;5;241m=\u001b[39m SimpleDirectoryReader(\n\u001b[0;32m      9\u001b[0m     input_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompany_policies\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m )\u001b[38;5;241m.\u001b[39mload_data()\n",
      "\u001b[1;31mTypeError\u001b[0m: Can't instantiate abstract class ArkLLM without an implementation for abstract methods 'achat', 'acomplete', 'astream_chat', 'astream_complete', 'chat', 'metadata', 'stream_chat', 'stream_complete'"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'your APIkey'\n",
    "\n",
    "# 1. åŠ è½½æ–‡æ¡£ï¼ˆæ”¯æŒ docx / pdf / txt / mdï¼‰\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_dir=\"company_policies\"\n",
    ").load_data()\n",
    "\n",
    "# 2. æ„å»ºå‘é‡ç´¢å¼•ï¼ˆEmbedding + Node parsingï¼‰\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# 3. åˆ›å»ºæŸ¥è¯¢å¼•æ“\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# 4. æé—®ç¤ºä¾‹ï¼šè‡ªåŠ¨æ£€ç´¢ç›¸å…³åˆ¶åº¦å¹¶æ€»ç»“å›ç­”\n",
    "response = query_engine.query(\n",
    "    \"å…¬å¸çš„åŠ ç­è®¡ç®—è§„åˆ™æ˜¯ä»€ä¹ˆï¼Ÿæœ‰å“ªäº›ç‰¹åˆ«è¯´æ˜ï¼Ÿ\"\n",
    ")\n",
    "\n",
    "print(\"AI å›ç­”ï¼š\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d8a070",
   "metadata": {},
   "source": [
    "## 4. Polars + NLPï¼šä½ ä¸çŸ¥é“å´æœ€å¼ºçš„æ–‡æœ¬å¤„ç†ç»„åˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798d8447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.read_csv('comments.csv')\n",
    "\n",
    "keywords = ['python', 'api', 'programming', 'script', 'automation']\n",
    "filtered = df.filter(\n",
    "    pl.col('comment').str.contains('|'.join(keywords), literal=False)\n",
    ")\n",
    "\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f166230a",
   "metadata": {},
   "source": [
    "## 5. KeyBERTï¼šåŸºäº Transformer çš„å…³é”®è¯æå–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a92115e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('python', 0.7005), ('automation', 0.6758), ('simple', 0.2128)]\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(\n",
    "    'Python makes automation incredibly simple.',\n",
    "    top_n=3\n",
    ")\n",
    "\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8e33bf",
   "metadata": {},
   "source": [
    "## 6. Transformersï¼šæ— æ‰€ä¸èƒ½çš„ NLP å·¥å…·ç®±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd2164e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0412d90a18d24dc8b540721263fc3391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\admin\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffcf519ad2e4155bb9dcd82698b5a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc917fe3fed451b9a4e043ad19171cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6912e9aa56a0486a9acfa9eaac09bcfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e70e5ef93746709843f4fc73058e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'This bug caused our production server to crash.', 'labels': ['software issue', 'support', 'feature request', 'finance'], 'scores': [0.7345870733261108, 0.14557555317878723, 0.07310790568590164, 0.04672941565513611]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('zero-shot-classification')\n",
    "result = classifier(\n",
    "    \"This bug caused our production server to crash.\",\n",
    "    candidate_labels=['finance', 'software issue', 'support', 'feature request']\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
